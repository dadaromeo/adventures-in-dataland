{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Previously](https://dadaromeo.github.io/the-voice-tweets-mining-part-2), we explored a model that exploits the links between the entities to help us find the key players in the data. Here, we will focus on the tweetâ€™s text to better understand what the users are talking about. We move away from the network model weâ€™ve used previously and discuss other methods for text analysis. We first explore [*topic modeling*](https://en.wikipedia.org/wiki/Topic_model), an approach that finds natural topics within the text. We then move on to [sentiment analysis](https://dadaromeo.github.io/the-voice-afrique-tweets-mining-part-4), the practice of associating a document with a sentiment score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding topics\n",
    "\n",
    "The data we collected from Twitter is a relatively small sample, but attempting to read each individual tweet is a hopeless cause. A more reachable goal is to get a high-level understanding of what users are talking about. One way to do this is by understanding the topics the users are discussing in their tweets. In this section we discuss the automatic discovery of topics in the text through *topic modeling* with [Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (LDA), a popular topic modeling\n",
    "algorithm.\n",
    "\n",
    "Every topic in LDA is a collection of words. Each topic contains all of the words in the corpus with a probability of the word belonging to that topic. So, while all of the words in the topic are the same, the weight they are given differs between topics.\n",
    "\n",
    "LDA finds the most probable words for a topic, associating each topic with a theme is left to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA with Gensim\n",
    "\n",
    "To perfom the LDA computation in Python, we will use the `gensim` library ([topic modeling for human](https://radimrehurek.com/gensim/)). As we can see, most of the work is done for us, the real effort is in the preprocessing of the documents to get the documents ready. The preprocessing we will perfom are:\n",
    "\n",
    "- **Lowercasing** - Strip casing of all words in the document (i.e: `\"@thevoiceafrique #TheVoiceAfrique est SUPERB! :) https://t.co/2ty\"` becomes `\"#thevoiceafrique est superb! :) https://t.co/2ty\"`)\n",
    "\n",
    "\n",
    "- **Tokenizing** - Convert the string to a list of tokens based on whitespace. This process also removes punctuation marks from the text. This becomes the list `[\"@thevoiceafrique\", \"#thevoiceafrique\", \"est\" \"superb\", \":)\", \"https://t.co/2ty\"]`\n",
    "\n",
    "\n",
    "- **Stopword Removal** - Remove *stopwords*, words so common that their presence does not tell us anything about the dataset. This also removes smileys, emoticons, mentions hashtags and links: `[\"@thevoiceafrique\", \"#thevoiceafrique\", \"superb\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import emoji\n",
    "from twitter.parse_tweet import Emoticons\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import TextCorpus\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "host = \"localhost\"\n",
    "port = 27017\n",
    "\n",
    "db = MongoClient(host, port).search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `stopwords-fr.txt` file is downloaded [here](https://github.com/stopwords-iso/stopwords-fr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = set()\n",
    "stop_words.update(list(string.punctuation))\n",
    "stop_words.update(stopwords.words(\"french\"))\n",
    "stop_words.update(Emoticons.POSITIVE)\n",
    "stop_words.update(Emoticons.NEGATIVE)\n",
    "stop_words.update([\"â€™\", \"â€¦\", \"ca\", \"Â°\", \"Ã§Ã \", \"Â»\", \"Â«\", \"â€¢\", \"the\",\n",
    "                    \"voice\", \"afrique\", \"voix\", \"â€“\", \"::\", \"â€œ\", \"â‚©\", \"ðŸ¤£\"])\n",
    "\n",
    "with open(\"data/stopwords-fr.txt\") as f:\n",
    "    stop_words.update(map(str.strip, f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenize = TweetTokenizer().tokenize\n",
    "\n",
    "def parse(text):\n",
    "    \n",
    "    text = text.strip()\n",
    "    text = text.strip(\"...\")\n",
    "    found = emoji.demojize(text).split(\" \")\n",
    "    text = \" \".join([t for t in found if not(\"_\" in t)])\n",
    "    text = \" \".join(re.split(r\"\\w*\\d+\\w*\", text)).strip()\n",
    "    tokens = tokenize(text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        cond = (token.startswith((\"#\", \"@\", \"http\", \"www\")) or\n",
    "                \".\" in token or\n",
    "                \"'\" in token\n",
    "                )\n",
    "                \n",
    "        if not(cond):\n",
    "            yield token\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    for token in parse(text):\n",
    "        if not(token in stop_words):\n",
    "            yield token\n",
    "\n",
    "class Corpus(TextCorpus):\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    \n",
    "    def get_texts(self):\n",
    "        for tweet in self.input:\n",
    "            tweet = preprocess(tweet)\n",
    "            yield list(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = [tweet[\"text\"] for tweet in db.thevoice.find() if not(\"retweeted_status\" in tweet.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regexp = emoji.get_emoji_regexp().findall\n",
    "\n",
    "for tweet in tweets:\n",
    "    stop_words.update(regexp(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2942\n",
      "Number of tokens: 4050\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(tweets)\n",
    "\n",
    "print(\"Number of documents: {}\\nNumber of tokens: {}\".format(len(corpus), len(corpus.dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the choice of the number of topics is arbitrary\n",
    "\n",
    "lda = LdaModel(corpus, num_topics=25, id2word=corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_topics(n, n_words=10, fmt=\"simple\"):\n",
    "    \"\"\"Show `n` randomly selected topics and thier\n",
    "    top words.\n",
    "    \"\"\"\n",
    "    from tabulate import tabulate\n",
    "    \n",
    "    topics = {}\n",
    "    ids = np.arange(lda.num_topics)\n",
    "    ids = np.random.choice(ids, n, replace=False)\n",
    "    for i in ids:\n",
    "        topic = lda.show_topic(i, n_words)\n",
    "        words,prop = zip(*topic)\n",
    "        topics[i+1] = list(words)\n",
    "    \n",
    "    tabular = tabulate(topics, headers=\"keys\", tablefmt=fmt)\n",
    "    \n",
    "    print(tabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1         20         21         22         24           9           10       11         12        14\n",
      "--------  ---------  ---------  ---------  -----------  ----------  -------  ---------  --------  --------\n",
      "singuila  chante     go         asalfo     lokua        nadia       nadia    shayden    gars      chante\n",
      "mdrrr     congolais  charlotte  amy        out          fin         deh      chansons   frÃ¨re     chanson\n",
      "connais   musique    locko      nadia      aime         titres      coach    mots       ans       jackson\n",
      "grave     locko      ndutu      place      candidat     ans         asalfo   candidats  grande    vraiment\n",
      "fille     voir       singuila   faut       grand        putain      mieux    grave      ariana    singui\n",
      "pd        normal     arrÃªtez    buzz       maman        choix       tuer     coachs     juges     dÃ©jÃ \n",
      "appuyer   lien       direct     retourne   famille      go          gar      aventure   petit     sort\n",
      "bidi      pourtant   asalfo     winehouse  lokoua       prestation  appuyÃ©   cest       commence  love\n",
      "asalfo    jaime      hey        chante     concurrence  bonne       connait  yeux       grÃ¢ce     marie\n",
      "chanson   savais     serieux    chanter    faut         buzzer      lol      vois       hamilton  truc\n"
     ]
    }
   ],
   "source": [
    "show_topics(10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above show the distribution of words within the different topics. From that, we can see that viewers are talking about the different candidates and coaches. In the [next](https://dadaromeo.github.io/the-voice-afrique-tweets-mining-part-3) post, we will use *Sentiment Analysis* to see if we see what sentiment is the most present in the data.e\n",
    "\n",
    "Thanks for following."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
